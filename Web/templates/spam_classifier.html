{% extends 'base.html' %}

{% block navMenueHeader %}
	<nav class="navbar" id="navbartop">
    <div class="navbar_container">
      <div class="nav-left">
      <ul>
        <li><a href="/">Home</a></li>
        <li><a href="/#projects">Projects</a></li>
        <li><a href="/#skills">Skills</a></li>
        <li><a href="/contact">Contact</a></li>
      </ul>
      </div>

      <div class="nav-right">
      <ul>
              <li><span id="datetime"></span></li>
          </ul>
      </div>
    <script>
    var dt = new Date();   
    const monthLst = ['January', 'February', 'March', 'April', 'May', 'June',
                   'July', 'August', 'September', 'October', 'November', 'December']

    const dayLst = ['Sunday', 'Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday']
    var month = monthLst[dt.getMonth()]
    var dayofWeek = dayLst[dt.getDay()]
    var year = dt.getFullYear();
    var dayofMonth = dt.getDate();
    dayofMonth = dayofMonth.toString();
    year = year.toString();
    var printingDate =dayofWeek +", " + month + "-" + dayofMonth + "-" + year;
    document.getElementById('datetime').innerHTML=printingDate;
    </script>

    </div>
  </nav>
{% endblock %}



{% block navMenue %}
	<nav class="navbar">
		<div class="navbar_container">
			<ul>
				<li><a class="current" href="#">Project Description</a></li>
				<li><a href="/test_spam">Running Project</a></li>
			</ul>
		</div>
	</nav>
{% endblock %}



{% block content %}
<br>            
<p style="font-size: 28px; font-weight: bold; color: #0000CD; text-align: center; margin-bottom: 40px;">Text Message Spam Classifier</p>
<section id="home-info" class="ham_spam">
<div class="ham_spam"> 
<h1>Introduction</h1>
<p>Nowadays, spam detection is one of the major applications of Machine Learning. As an example, email service providers use this technique to detect spam email (Junk mail) and prevent them to be accessed in male box. Like email services, we can use spam detection technique for analyzing short message service (SMS) as well. Before getting into the project, let us first define what is a spam message. They are unwanted or unexpected messages that shown up in user cell phone. Normally, they are not only annoying, but also can be dangerous too. For instance, they can contain some links that lead to phishing web sites or similarly websites that are hosting malware. Since the spam messages are sent from unknown phone numbers, they are trying to be more eye-catching than usual messages. For example, it is likely to have words such as win, chance, prize and so on. To detect spam messages, we can take advantage of supervised machine learning algorithms if we have enough labeled dataset. <br>
In this project, we design a machine learning model that can efficiently predict if a text message is spam or not. In the following section, the detail of dataset is presented. Finally, this project is implemented in form of a machine learning web applications. So, any user can <a href="/test_spam" class='run_project'style="font-weight: bold;font-style: italic;font-size: 15px;">run this project for a given message.</a></p>
<h1>Dataset:</h1>
<p>We use UCI machine learning repository for the SMS data set which available <a href="https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection" target="_blank"> here</a>. The dataset is made of real 5574 messages. According to the UCI website, the data are gathered from different sources as follow:</p>

<ul>
<li>A collection of 425 SMS spam messages from the Grumbletext Web site - a UK forum in which cell phone users make public claims about SMS spam messages. </li>
<li>A subset of random 3,375 SMS messages of the NUS SMS Corpus (NSC), which is a dataset of for research at the Department of Computer Science at the National University of Singapore.</li>
<li>A list of 450 SMS ham messages collected from Caroline Tag's <a href="https://etheses.bham.ac.uk/id/eprint/253/1/Tagg09PhD.pdf" target="_blank"> PhD Thesis.</a></li>
<li>1002 SMS ham messages and 322 spam messages gathered from <a href="http://esp.uem.es/" target="_blank"> this link</a></li>
</ul>
<h1>Explanatory Data Analysis:</h1>
<p>As it is observable in Figure 1, 13.4% of messages in the data set are spam and 86.6% are ham(normal). It is clear that we face with an imbalanced dataset. But it is not strongly imbalanced, like dataset we can expect in other applications for example fraud transaction dataset.</p>
   
<figure class='image_div'>
	<img src="/static/images/spam_classifier/img_distribution_pie.png" alt="distribution of message" style="min-width: 400px; max-width: 400px;min-height: 222px;max-height: 222px; margin-left: auto; margin-right: auto;">
	  <figcaption>Figure 1. Distribution of messages types; Normal(or ham) messages vs spam messages.</figcaption>
</figure>


<br>
  <h1>Part of Speech (Pos) analysis:</h1>
  <p>Figure 2 and Figure 3 show the result of part of speech analysis. The “Proper Noun” ratio shows the maximum difference between two types of messages. Indeed, 32.2% of words in Spam messages are Proper Noun while this ratio in ham messages is less than 27%. This difference can be addressed in the concept of Spam messages where one can expect existence of <strong>“brand”</strong> names as the goal behind of any spam messages.  In addition, Spam messages have more verb than ham messages. Regarding the verb ratio comparison, one scenario can be the fact that in normal messages we can expect some sequence of messages as a two-sided conversation where verb can be dropped since people already know the subject. In contrast, in spam messages, we highly expect one-sided communication where verb is required to convey the subject. Furthermore, these two figures show that the adverb and adjective ratio in ham messages are more than spam messages. </p>
  
  <figure class='image_div'>
	<img src="/static/images/spam_classifier/img_postag_pie.png" alt="Part_Of_Speach" style="min-width: 799px; max-width: 799px;min-height: 279px;max-height: 279px; margin-left: auto; margin-right: auto;">
	<figcaption>Figure 2. Part of speach comparison, ham messages versus spam messages.</figcaption>
  </figure>
  
  <br>
  <figure class='image_div'>
	<img src="/static/images/spam_classifier/img_wordtag_bar.png" alt="Word_Tag" style=" max-width: 611px;max-height: 422px; margin-left: auto; margin-right: auto;">
	<figcaption>Figure 3. Part of speach distribution, ham messages versus spam messages.</figcaption>
  </figure>

<br>
<h1>Word Cloud:</h1>
<p>Figure 4 and Figure 5 show the word cloud analysis of ham and spam messages, respectively. It is observable that in top frequent words in ham messages are words like:
<ul>
<li>“get, go, got, come, like, love, home, ok, know, etc.”</li>
</ul>
which makes sense as part of everyday conversation.
In contrast, Figure 5 shows that top frequent words in spam messages are:
<ul>
	<li>“free, call, text, stop, mobile, urgent, customer service, etc”</li>
</ul>
Unlike frequent words in ham messages, this set of words are not assumed as usual words in everyday conversation.</p>

<figure class='image_div'>
<img src="/static/images/spam_classifier/img_ham_wcloud.png" alt="Ham_Messages_Word_Cloud" style=" max-width: 611px;max-height: 422px; margin-left: auto; margin-right: auto;">
<figcaption>Figure 4. Word cloud of ham messages.</figcaption>
</figure>

<figure class='image_div'>
<img src="/static/images/spam_classifier/img_spam_wcloud.png" alt="Spam_Messages_Word_Cloud" style=" max-width: 611px;max-height: 422px; margin-left: auto; margin-right: auto;">
<figcaption>Figure 5. Word cloud of spam messages.</figcaption>
</figure>


<br>
<h1>Feature Engineering:</h1>
<p>In Natural Language Processing (NLP) based applications, techniques like Bag of Words are likely to be very effective. However, extracting new features from dataset always can provide us new intuitive knowledge. To extract new features, we have to think about the concept and the goal of the project. In this project, we require to take into account possible indicators that can help us to distinguish normal messages from spam messages. In this case, the following features are extracted from text messages.</p>

<h1>Category 1, Numerical Features:</h1>
<ul>
	<li>token_count: Number of words in message</li>
	<li>avg_word_length: Average length of words (characters) in message</li>
	<li>upper_count: Count of uppercase words</li>
	<li>stop_count: Count of stopwords</li>
	<li>num_count: Count of numerical words</li>
	<li>punc_count: Count of punctuations in message</li>
</ul>


<h1>Category 2, Binary Features:</h1> 
<ul>
<li>hasNumber: Whether message has any digits</li>
<li>hasPhone: Whether message has any phone number</li>
<li>hasCurrency: Whether message has any currency symbols</li>
</ul>

<h1>Category 3, Features over other features:</h1>
<ul>
<li>sum_has_features: = hasNumber + hasCurrency + hasPhone + hasURL</li>
<li>sum_count_features = num_count + upper_count + punc_count</li>
<li>pos_tag: ratio of (Proper Noun + Verb) – ratio of (Adjective + Pronoun + Adverb)</li>
</ul>

<br>
<h1>Feature Importance:</h1>
<p>Figure 6, shows the feature importance that extracted from random forest classifier model. It is observable that top 5 important features are included:
sum_has_features, hasPhone, avg_word_length, num_count, pos_tag.<br> 
It is interesting that the most important feature based on random forest model is sum_has_features which itself is made over four engineered features.</p>

<figure class='image_div'>
<img src="/static/images/spam_classifier/img_importance_bar.png" alt="Feature_Importance" style=" max-width: 611px;max-height: 422px; margin-left: auto; margin-right: auto;">
<figcaption>Figure 6. Feature importance visualization extracted by random forest classifier.</figcaption>
</figure>

<br>
<h1>Explanatory Analysis of Engineered Features</h1>
<p>Full analysis of all engineered features is available in my <a href="https://github.com/tednaseri" target="_blank"> GitHub Repository</a>. But in summary the following figures represents some comparisons between ham and spam class. For instance, word count histogram in Figure 7 shows that ham messages are made by less words than spam messages. This makes sense since normal text messages can be part of a possibly longer two-sided conversation. While for spam messages it is not the case. In a spam message, to convey the concept, senders usually use more words than normal messages. 
<br>Figure 8 shows that word length in spam messages is longer than ham messages. This difference can be addressed by considering the fact that in everyday text message conversation people tend to use abbreviation as a habit. While it is not the same case in spam messages.
</p>

<figure class='image_div'>
<img src="/static/images/spam_classifier/img_word_count_hist.png" alt="Word_Count_Histogram" style=" max-width: 611px;max-height: 422px; margin-left: auto; margin-right: auto;">
<figcaption>Figure 7. Comparison of word count in messages.</figcaption>
</figure>

<br>
<figure class='image_div'>
<img src="/static/images/spam_classifier/img_word_length_box.png" alt="Word_Length_Average" style=" max-width: 611px;max-height: 382px; margin-left: auto; margin-right: auto;">
<figcaption>Figure 8. Comparison for the average length of words in messages.</figcaption>
</figure>

<p>Figure 9 and Figure 10 show the difference in terms of existence of any numerical word and phone number is messages respectively. So based on these extracted features, it is found that:
	<ul>
		<li>11.5 % of normal messages have digits, while more than 86% of spam messages contain some kind of digits.</li>
		<li>None of normal messages include phone number, while 57.3% of spam messages have phone numbers. High percentage of spam messages including phone number is absolutely meaningful, since they normally ask receiver to contact the sender.</li>
	</ul>
</p>


<figure class='image_div'>
<img src="/static/images/spam_classifier/img_has_numerical_bar.png" alt="Digits_Existence_Comparison" style=" max-width: 699px;max-height: 275px; margin-left: auto; margin-right: auto;">
<figcaption>Figure 9. Comparison of existence of any digits in messages.</figcaption>
</figure>

<br>
<figure class='image_div'>
<img src="/static/images/spam_classifier/img_has_phone_bar.png" alt="PhoneNumber_Existence_Comparison" style=" max-width: 699px;max-height: 275px; margin-left: auto; margin-right: auto;">
<figcaption>Figure 10. Comparison of existence of any phone number in messages.</figcaption>
</figure>



<h1>Modelling:</h1>
<p>To model, we apply 4 sets of modeling. Set1, Set2, Set3, and Set4. For each model, we split the dataset randomly into 80% train and 20% test data. And during training of each model, cross validation and hyperparameter tunning is applied on training dataset. Before prediction, models do not touch test dataset. For the comparison purposes, four metrics of Accuracy, Precision, Recall, and F1 are calculated and reported.
<h1>Set1:</h1>
In Set1, we use the following models only on engineered features that extracted from messages:
<ul>
<li>Selected Vector Machine,</li>
<li>Logistic Regression,</li>
<li>Random Forest Classifier,</li>
<li>XGBoost Classifier</li>
</ul>
For each model, the related hyperparameters have been tunned along with cross validation to provide more general predictability. The results are presented in Figure 11. It is clear that the engineered features work very well. We could reach very high <strong>accuracy of 98.47%</strong> by XGBoost model. In terms of accuracy and precision the three models of XGBoost, Random Forest, and SVC work similar. By taking Recall metric into account, <strong>XGBoost shows itself as the best model</strong> on engineering features.</p>

<figure class='image_div'>
<img src="/static/images/spam_classifier/model1.jpg" alt="Model_Comparison_Set1" style=" max-width: 800px;max-height: 340px; margin-left: auto; margin-right: auto;">
<figcaption>Figure 11. Model Comparison, Set1, using only engineered features extracted from text.</figcaption>
</figure>

<br>
<h1>Modeling Set2, using bag of word:</h1>
<p>For this set, bag of word as the input dataset has been used. And also all models used in in set1 along with multinomial naive bayes are tested. Again, hyperparameters of all models are tunned. The result is presented in Figure 12. Using bag of words, naïve bayes model shows itself as the best model where we could reach <strong>accuracy of 99.55%</strong>. In addition, we could obtain <strong>100% precision</strong> which is really great.</p>


<figure class='image_div'>
<img src="/static/images/spam_classifier/img_model_Set2_bar.png" alt="Model_Comparison_Set2" style=" max-width: 800px;max-height: 330px; margin-left: auto; margin-right: auto;">
<figcaption>Figure 12. Model Comparison, Set2, using bag of words as dataset.</figcaption>
</figure>

<br>
<p>The accuracy comparison between modeling Set1 and Set2 are presented in Figure 12. The metric values are relatively close to each other. However, it is clear that the best model is naïve bayes model using bag of words. For clarification purposes, it should be noted that in Set1, as a result of applying standard scaler, we have some negative values in dataset. Because of that naïve bayes model was ignored in Set 1 and we applied it only within Set2.</p>

<figure class='image_div'>
<img src="/static/images/spam_classifier/img_model_comparison_bar.png" alt="Accuracy_Comparison" style=" max-width: 800px;max-height: 340px; margin-left: auto; margin-right: auto;">
<figcaption>Figure 13. Accuracy comparison between Set1 and Set2.</figcaption>
</figure>

<br>
<h1>Modeling Set3</h1>
<p>In Set 3, all engineered features were joined into bag of words. Afterwards, models are tested using this new dataset. By the way, we could not reach improvement in business metrics. However, again the naïve bayes was the best model.</p>
<h1>Modeling Set4:</h1>
<p>In Set 4, first naïve bayes model used for training on the bag of words. Then it used to predict the probability of messages. And the probability results were added into engineered features dataset. Afterwards all different models were checked. In this set, we could improve the business metrics. But the results are obtained the same as the results in Set2. Since the business metrics in Set2 and Set4 showed the same results, the naïve bayes model of Set2 is chosen as the best model. The reason behind that is the fact that in Set2 the model used a simpler dataset.</p>


<h1>Undersampling</h1>
<p>As about 14% of the dataset are spam messages, we tried undersampling of majority class to check the impact of resampling. In undersampling technique, we keep all minority class. But randomly we chose some of the data from majority class in a way to assure the optional ratio between the minor and majority class. We have checked different ratio. For instance, for 25% ratio: 
	<ul>
<li>Majority class (ham messages):  75%</li>
<li>Minority class (spam messages): 25%</li>
</ul>
The result is presented in Figure 14. It is clear that by applying undersampling, we could improve the recall metrics while we see a decreasing in precision. To decide if we need undersampling or not, it depends on business goal. In this project since detecting of normal messages are more important than spam messages, precision is more important. But suppose, if we study fraud transaction detection, then increasing recall metric is absolutely preferable. For now, we do not apply undersampling in this project.</p>

<figure class='image_div'>
<img src="/static/images/spam_classifier/img_undersample_bar.png" alt="Undersampling" style=" max-width: 650px;max-height: 300px; margin-left: auto; margin-right: auto;">
<figcaption>Figure 14. Impact of undersampling on business metrics.</figcaption>
</figure>


<h1>Conclusion:</h1>
<p>In this project, we designed a supervised binary classification machine learning model that can efficiently predict if a text message is spam or not. The applied feature engineering and explanatory data analysis provided a very good description about the differences between the normal(or ham) and spam messages. For example:
<ul>
<li>more than 57% of spam messages have phone number while almost none of normal messages have phone number.</li>
<li>Only 11.5% of normal messages include digits while this ratio is 86.7% in spam messages.</li>
</ul>
<strong>Naïve bayes</strong> model is found as the best model where we could reach business metrics as:</p>
<ul>
<li>Accuracy score: 0.995516</li>
<li>Precision score: 1.00</li>
<li>Recall score: 0.966216</li>
<li>F1 score: 0.982818</li>
</ul>


<h1>Running the Project:</h1>
<p>This project is implemented in form of a machine learning web applications. So, any user can <a href="/test_spam" class='run_project'style="font-weight: bold;font-style: italic;font-size: 15px;">run this project for a given message.</a></p>

</div>
</section>
{% endblock %}